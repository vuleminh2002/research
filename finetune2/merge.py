import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# ---------------- CONFIG ----------------
BASE_MODEL = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
LORA_DIR   = "/research/finetune2/tinyllama-geocode-lora_s2"
OUT_DIR    = "/research/finetune2/tinyllama-geocode-merged-bf16"

print("ðŸ§  Loading base model in BF16â€¦")
base = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.bfloat16,
    device_map={"": 0},  # H100
)

print("ðŸ”§ Loading LoRA adapterâ€¦")
model = PeftModel.from_pretrained(
    base,
    LORA_DIR,
    is_trainable=False,
    local_files_only=True,
)

print("ðŸ”— Merging LoRA into base weightsâ€¦")
model = model.merge_and_unload()        # after this, no PEFT

print("ðŸ’¾ Saving merged model to:", OUT_DIR)
model.save_pretrained(OUT_DIR, safe_serialization=True)

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
tokenizer.save_pretrained(OUT_DIR)

print("âœ… Done. Merged model saved.")
